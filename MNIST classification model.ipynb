{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "The MNIST classification refers to handwritten digit recognition. The MNIST dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "This model detects which digit is written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n",
    "\n",
    "#extracting the training and testing dataset with the built references\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# by default, TF has training and testing datasets, but no validation sets\n",
    "# thus we must split it on our own\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# casting this number to an integer\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "\n",
    "# scaling our data in some way to make the result more numerically stable\n",
    "# in this case the inputs are preferred in between 0 and 1\n",
    "def scale(image, label):\n",
    "    # we make sure the value is a float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)\n",
    "    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 \n",
    "    image /= 255.\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "\n",
    "# let's also shuffle the data\n",
    "BUFFER_SIZE = 10000\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# making batches in training data\n",
    "BATCH_SIZE = 150\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 200\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "That's where we train the model we have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "360/360 - 4s - loss: 0.3054 - accuracy: 0.9132 - val_loss: 0.1430 - val_accuracy: 0.9578\n",
      "Epoch 2/5\n",
      "360/360 - 2s - loss: 0.1170 - accuracy: 0.9658 - val_loss: 0.1000 - val_accuracy: 0.9708\n",
      "Epoch 3/5\n",
      "360/360 - 2s - loss: 0.0786 - accuracy: 0.9770 - val_loss: 0.0771 - val_accuracy: 0.9792\n",
      "Epoch 4/5\n",
      "360/360 - 2s - loss: 0.0575 - accuracy: 0.9832 - val_loss: 0.0594 - val_accuracy: 0.9848\n",
      "Epoch 5/5\n",
      "360/360 - 2s - loss: 0.0433 - accuracy: 0.9863 - val_loss: 0.0513 - val_accuracy: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a8b0abca90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the maximum number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets),validation_steps=1, verbose =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 1s 708ms/step - loss: 0.0752 - accuracy: 0.9777"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08. Test accuracy: 97.77%\n"
     ]
    }
   ],
   "source": [
    "# We can apply some nice formatting if we want to\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
